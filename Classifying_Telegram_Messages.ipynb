{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying_Telegram_Messages.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWG2t2Bd5yK8"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install emoji\n",
        "!pip install langdetect\n",
        "!pip install transformers\n",
        "!pip install ekphrasis\n",
        "!pip install -U datasets\n",
        "!pip install pymongo_ssh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaylrY3c6AvO"
      },
      "source": [
        "There are some helper functions written in some external python files. We load these python files directly into colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "Jru6IWDe57dN",
        "outputId": "80121bde-2358-4c5d-cd42-8e8524c3b3a0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8485941d-f280-486e-96a3-dcea459e37d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8485941d-f280-486e-96a3-dcea459e37d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving inferencing.py to inferencing.py\n",
            "Saving model_params.py to model_params.py\n",
            "Saving preprocessing.py to preprocessing.py\n",
            "Saving training.py to training.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'inferencing.py': b'import hashlib \\nimport pymongo\\nimport torch\\nimport math\\n\\nfrom pymongo_ssh import MongoSession\\n\\n\\n# connect to database\\ndef connect_to_database(key, host=\\'138.246.233.159\\'):\\n    session = MongoSession(\\n    host=host,\\n    port=22,\\n    user=\\'ubuntu\\',\\n    key=key,\\n    uri=\\'mongodb://127.0.0.1:27017\\')\\n    db = session.connection[\\'telegram\\']\\n    \\n    db_channels = db.channels\\n    db_messages = db.messages\\n    db_errors = db.errors\\n    db_users = db.users\\n    db_entities = db.entities\\n    \\n    return db_channels, db_messages, db_errors, db_users, db_entities\\n\\ndef hashContent(field1,field2,field3):\\n    to_hash = field1 if field1 is not None else \\'\\'\\n    if field2 is not None:\\n        to_hash = to_hash + field2\\n    if field3 is not None:\\n        to_hash = to_hash + field3\\n    result = hashlib.md5(to_hash.encode()) \\n    return result.hexdigest()\\n\\ndef predict(text,tokenizer,model):\\n    inputs =  tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(\\'cuda\\')\\n    labels = torch.tensor([1]).unsqueeze(0).cuda()\\n    outputs = model(**inputs, labels=labels)\\n    m = torch.nn.Softmax(dim=1).cuda()\\n    # softmax the logits\\n    softmaxed = m(outputs.logits).detach().cpu().numpy()\\n    # get the probaility of classes\\n    # 0 ngeative (neutral)\\n    # 1 positive (hate, offense, attack)\\n    return softmaxed[0]',\n",
              " 'model_params.py': b\"model_params = {'germeval_1819': {\\n                    'train':'germeval_1819_train.csv',\\n                    'validation':'germeval_1819_valid.csv',\\n                    'test':'germeval_1819_test.csv',\\n                    'label':'label1',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'OFFENSE':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'OFFENSE'\\n                    }},\\n                'germeval_18': {\\n                    'train':'germeval_18_train.csv',\\n                    'validation':'germeval_18_valid.csv',\\n                    'test':'germeval_18_test.csv',\\n                    'label':'label1',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'OFFENSE':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'OFFENSE'\\n                    }},\\n                'germeval_19': {\\n                    'train':'germeval_19_train.csv',\\n                    'validation':'germeval_19_valid.csv',\\n                    'test':'germeval_19_test.csv',\\n                    'label':'label1',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'OFFENSE':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'OFFENSE'\\n                    }},\\n                'germeval_1819_task_2': {\\n                    'train':'germeval_1819_train.csv',\\n                    'validation':'germeval_1819_valid.csv',\\n                    'test':'germeval_1819_test.csv',\\n                    'label':'label2',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'PROFANITY':1,\\n                        'INSULT':2,\\n                        'ABUSE':3\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'PROFANITY',\\n                        2:'INSULT',\\n                        3:'ABUSE'\\n                    }},\\n                'germeval_18_task_2': {\\n                    'train':'germeval_18_train.csv',\\n                    'validation':'germeval_18_valid.csv',\\n                    'test':'germeval_18_test.csv',\\n                    'label':'label2',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'PROFANITY':1,\\n                        'INSULT':2,\\n                        'ABUSE':3\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'PROFANITY',\\n                        2:'INSULT',\\n                        3:'ABUSE'\\n                    }},\\n                'germeval_19_task_2': {\\n                    'train':'germeval_19_train.csv',\\n                    'validation':'germeval_19_valid.csv',\\n                    'test':'germeval_19_test.csv',\\n                    'label':'label2',\\n                    'mapping': {\\n                        'OTHER':0,\\n                        'PROFANITY':1,\\n                        'INSULT':2,\\n                        'ABUSE':3\\n                    },\\n                    'mapping_reverse': {\\n                        0:'OTHER',\\n                        1:'PROFANITY',\\n                        2:'INSULT',\\n                        3:'ABUSE'\\n                    }},\\n                'covid_2021': {\\n                    'train':'covid_2021_training.csv',\\n                    'validation':'covid_2021_validation.csv',\\n                    'test':'covid_2021_test.csv',\\n                    'label':'label',\\n                    'mapping': {\\n                        'not':0,\\n                        'abusive':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'not',\\n                        1:'abusive'\\n                    }},\\n               'hasoc_1920':{\\n                    'train':'hasoc19_20_german_train.tsv',\\n                    'validation':'hasoc19_20_german_dev.tsv',\\n                    'test':'hasoc19_20_german_test.tsv',\\n                    'label':'task_1',\\n                    'mapping': {\\n                        'NOT':0,\\n                        'HOF':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'NOT',\\n                        1:'HOF'\\n                    }},\\n                'hasoc_1920_task_2':{\\n                    'train':'hasoc19_20_german_train.tsv',\\n                    'validation':'hasoc19_20_german_dev.tsv',\\n                    'test':'hasoc19_20_german_test.tsv',\\n                    'label':'task_1',\\n                    'mapping': {\\n                        'NONE':3,\\n                        'PRFN':1,\\n                        'OFFN':2,\\n                        'HATE':0\\n                    },\\n                    'mapping_reverse': {\\n                        3:'NONE',\\n                        1:'PRFN',\\n                        2:'OFFN',\\n                        0:'HATE'\\n                    }},\\n                'hasoc_2020_task_2':{\\n                    'train':'hasoc2020_german_train.tsv',\\n                    'validation':'hasoc2020_german_valid.tsv',\\n                    'test':'hasoc2020_german_test.tsv',\\n                    'label':'task_1',\\n                    'mapping': {\\n                        'NONE':3,\\n                        'PRFN':1,\\n                        'OFFN':2,\\n                        'HATE':0\\n                    },\\n                    'mapping_reverse': {\\n                        3:'NONE',\\n                        1:'PRFN',\\n                        2:'OFFN',\\n                        0:'HATE'\\n                    }},\\n                'hasoc_2020':{\\n                    'train':'hasoc2020_german_train.tsv',\\n                    'validation':'hasoc2020_german_valid.tsv',\\n                    'test':'hasoc2020_german_test.tsv',\\n                    'label':'task_1',\\n                    'mapping': {\\n                        'NOT':0,\\n                        'HOF':1\\n                    },\\n                    'mapping_reverse': {\\n                        0:'NOT',\\n                        1:'HOF'\\n                    }}}\",\n",
              " 'preprocessing.py': b'import emoji\\nimport numpy as np\\nimport re\\nfrom langdetect import detect\\nimport pandas as pd \\n\\nimport gensim\\n\\nfrom nltk.corpus import stopwords\\nstop_words_ger = stopwords.words(\\'german\\')\\nstop_words_en = stopwords.words(\\'english\\')\\n\\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\\nfrom ekphrasis.dicts.emoticons import emoticons\\n\\nimport spacy\\nnlp = spacy.load(\"de_core_news_sm\")\\n\\ndef prepare_single_text(text):\\n\\n    text = remove_hand_selected_words(text)    \\n    text = rermove_repeating_chars(text)\\n    text = emoji_2_text(text)\\n    text= \" \".join(tw_process.pre_process_doc(text))\\n    text = remove_special_chars(text)\\n    #text = text.lower()\\n    text = remove_white_spaces(text)\\n\\n    return(text)\\n\\t\\ndef prepare_single_text_without_ekphrasis(text):\\n\\n    text = remove_hand_selected_words(text)    \\n    text = rermove_repeating_chars(text)\\n    text = emoji_2_text(text)\\n    text= remove_usernames(text)\\n    text = remove_URLs(text)\\n    text = remove_special_chars(text)\\n    #text = text.lower()\\n    text = remove_white_spaces(text)\\n\\n    return(text)\\n\\n\\ndef remove_usernames(string):\\n    return re.sub(\\'@\\\\S+\\', \\'<name>\\', string)\\t\\n\\ndef remove_URLs(string):\\n    return re.sub(\\'http[s]?://\\\\S+\\', \\'<url>\\', string)\\n\\ndef get_text_processor():\\n    return(TextPreProcessor(\\n        normalize=[\\'url\\', \\'email\\', \\'percent\\', \\'money\\', \\'phone\\', \\'user\\', \\'time\\', \\'url\\', \\'date\\', \\'number\\'],\\n        \\n        segmenter=\"twitter\", \\n        corrector=\"twitter\", \\n        \\n        fix_html=True,  # fix HTML tokens\\n\\n        unpack_hashtags=False,  # perform word segmentation on hashtags\\n        unpack_contractions=False,  # Unpack contractions (can\\'t -> can not)\\n        spell_correct_elong=False,  # spell correction for elongated words\\n\\n        tokenizer=SocialTokenizer(lowercase=False).tokenize,\\n        dicts=[emoticons]\\n    ))\\ntw_process = get_text_processor()\\n\\ndef remove_numbers(s):\\n    return(re.sub(\" \\\\d+\", \" \", s))\\n\\ndef lemmatize_tokens(tokens):\\n    txt = \" \".join(tokens)\\n    doc = nlp(txt)\\n    return([token.lemma_ for token in doc])\\n\\ndef keepOnlyNounsAndNE(tokens):\\n    txt = \" \".join(tokens)\\n    doc = nlp(txt)\\n    \\n    keep = [str(w) for w in list(doc.ents)]\\n    for w in doc:\\n        if w.pos_ in [\\'NOUN\\']:\\n            keep.append(str(w.text))\\n    return([w for w in tokens if w in keep])\\n \\ndef make_bigrams(texts,bigram_mod):\\n    return [bigram_mod[doc] for doc in texts]\\n\\ndef make_trigrams(texts):\\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\\n\\ndef build_N_grams(text,threshold):\\n    # Build the bigram and trigram models\\n    bigram = gensim.models.Phrases(text, min_count=5, threshold=threshold) # higher threshold fewer phrases.\\n    trigram = gensim.models.Phrases(bigram[text], threshold=threshold)  \\n\\n    # Faster way to get a sentence clubbed as a trigram/bigram\\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\\n\\n    data_words_bigrams = make_bigrams(text, bigram_mod)\\n    return(data_words_bigrams)\\n\\ndef remove_stopwords(tweet):\\n    txt = []\\n    for w in tweet:\\n        if w in stop_words_ger:\\n            continue\\n            \\n        if w in stop_words_en:\\n            continue\\n        txt.append(w)\\n\\n    return(txt)\\n\\ndef remove_hand_selected_words(text):\\n    return(re.sub(r\"\\\\|lbr\\\\||\\\\|LBR\\\\||\\\\|AMP\\\\||&gt;|&amp;\", \" \", text))\\n\\ndef rermove_repeating_chars(text):\\n    to_remove = \".,?!\"\\n    return(re.sub(\"(?P<char>[\" + re.escape(to_remove) + \"])(?P=char)+\", r\"\\\\1\", text))\\n\\nfrom emoji import EMOJI_UNICODE, UNICODE_EMOJI\\nUNICODE_EMOJI = {v.encode(\\'unicode-escape\\').decode().replace(\"\\\\\\\\\", \"\").lower(): \"<\"+k.replace(\":\",\"\").replace(\\'-\\',\\'\\')+\">\" for k, v in EMOJI_UNICODE[\\'en\\'].items()}\\n\\ndef emoji_2_text(text):\\n    \\n    text = emoji.demojize(text, delimiters=(\"<\",\">\"))\\n    re_matches = re.findall(r\"(<U\\\\+[0-9a-zA-Z]*>)\", text)\\n    \\n     \\n    for emoji_unicode in re_matches:\\n        try:\\n            text = text.replace(emoji_unicode, UNICODE_EMOJI[re.sub(\\'[<>+]\\', \\'\\', emoji_unicode).lower()])\\n        except:\\n            text = text.replace(emoji_unicode,\"\")\\n    \\n        text = text.replace(emoji_unicode,\"\")\\n        \\n    m = re.search(\\'<[a-z_-]*(-)[a-z_-]*>\\',text)\\n    if m is not None:\\n        old_emoji = m.group(0)\\n        new_emoji = m.group(0).replace(\\'-\\',\\'_\\')\\n        text = text.replace(old_emoji,new_emoji)\\n    return(text)\\n\\ndef remove_special_chars(text):\\n    return(re.sub(r\"[^A-Za-z0-9\\\\s\\xc3\\xa4\\xc3\\xbc\\xc3\\x9f\\xc3\\xb6\\xc3\\x96\\xc3\\x84\\xc3\\x9c<>_:!?.,\\\\-]+\", \" \", text))\\n\\ndef detect_language(tw):\\n    try:\\n        return(detect(tw))\\n    except:\\n        return(\"unk\")\\n    \\ndef sentence_to_token(text):\\n    return([w.strip() for w in text.split()])\\n\\ndef token_to_sentence(token):\\n    return(\" \".join([w.strip() for w in token]))\\n\\ndef remove_white_spaces(text):\\n    return(\" \".join(text.split()))\\n',\n",
              " 'training.py': b\"import torch\\nimport torch.utils.data\\nimport preprocessing as pp\\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\\n\\nclass HateDataset(torch.utils.data.Dataset):\\n    def __init__(self, encodings, labels):\\n        self.encodings = encodings\\n        self.labels = labels\\n\\n    def __getitem__(self, idx):\\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\n        item['labels'] = torch.tensor(self.labels[idx])\\n        return item\\n\\n    def __len__(self):\\n        return len(self.labels)\\n\\n\\ndef compute_metrics(pred):\\n    labels = pred.label_ids\\n    preds = pred.predictions.argmax(-1)\\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\\n    acc = accuracy_score(labels, preds)\\n    f1_score_micro = f1_score(labels, preds, average='micro')\\n    f1_score_macro = f1_score(labels, preds, average='macro')\\n    f1_score_weighted = f1_score(labels, preds, average='weighted')\\n    return {\\n        'accuracy': acc,\\n        'precision': precision,\\n        'recall': recall,\\n        'f1': f1,\\n        'f1_micro': f1_score_micro,\\n        'f1_macro': f1_score_macro,\\n        'f1_weighted': f1_score_weighted,\\n    }\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOsBPjgk6ESa"
      },
      "source": [
        "The models are permanently stored on our Google Drive accounts. So we mount drive in order to access the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARSB4xSl6LQd",
        "outputId": "befb328f-ae88-4bfe-f495-4d164057b95c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hU6Ytzi6OBn",
        "outputId": "34cdc2e4-dca9-4889-cc12-007542690a31"
      },
      "source": [
        "import multiprocessing\n",
        "import pymongo\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "import operator\n",
        "import hashlib \n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import preprocessing as pp\n",
        "import inferencing as ig\n",
        "import model_params as mp\n",
        "import time\n",
        "import sys\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments,BertForSequenceClassification\n",
        "from bson.objectid import ObjectId\n",
        "from pymongo import MongoClient\n",
        "from pymongo import UpdateOne\n",
        "from tqdm import tqdm\n",
        "from spacy.language import Language\n",
        "from pandas import Panel\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPIv8fVv6TZC"
      },
      "source": [
        "db_channels, db_messages, db_errors, db_users, db_entities = ig.connect_to_database(key='/content/drive/My Drive/NLP/id_hsd')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8tdC9O86UMy"
      },
      "source": [
        "selected_dataset = 'hasoc_2020_task_2'\n",
        "MODEL_NAME = 'dbmdz/bert-base-german-cased'\n",
        "#MODEL_NAME = 'german-nlp-group/electra-base-german-uncased'\n",
        "#MODEL_NAME = 'deepset/gbert-base'\n",
        "max_length = 412\n",
        "workers = 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5hpv9oAVkU"
      },
      "source": [
        "Both Hasoc models have the following mapping 'NONE' -> 3, 'PRFN' -> 1, 'OFFN' -> 2, 'HATE' -> 0. So it does not follow the rule: \"The more offensive, the higher the index\". To fix this we have to map 'NONE' -> 0 and 'HATE' -> 3. Otherwise the inference would not work properly for long messages that have to be split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhf5RuX76eCA"
      },
      "source": [
        "def hasoc_multiclass_map(idx):\n",
        "  if idx == 0:\n",
        "    return 3\n",
        "  if idx == 3:\n",
        "    return 0\n",
        "  return idx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOO4fyVa6jrK"
      },
      "source": [
        "def inferenceMessages(df, queue_done, selected_dataset, lookup_table_hash_object_ids):\n",
        "    time.sleep(random.randint(0,30))\n",
        "    print(\"Worker has started with\", len(df))\n",
        "    # connect to database\n",
        "    db_channels, db_messages, db_errors, db_users, db_entities = ig.connect_to_database(key='/content/drive/My Drive/NLP/id_hsd')\n",
        "\n",
        "    # load classificaton model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = BertForSequenceClassification.from_pretrained(f\"/content/drive/My Drive/NLP/{selected_dataset}\")\n",
        "    model = model.cuda()\n",
        "    print(\"Model is loaded\")\n",
        "    \n",
        "    # sentence splitter\n",
        "    nlp = spacy.load(\"de_core_news_sm\")\n",
        "    @Language.component(\"set_custom_boundaries\")\n",
        "    def set_custom_boundaries(doc):\n",
        "        for token in doc[:-1]:\n",
        "            if token.text == \"...\":\n",
        "                doc[token.i + 1].is_sent_start = True\n",
        "        return doc\n",
        "    nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
        "    \n",
        "    \n",
        "    field_name = f\"hate_{selected_dataset}\"\n",
        "    \n",
        "    bulk = []\n",
        "    for index, row in df.iterrows():\n",
        "        combined_text = row['text']\n",
        "        if row['link_preview_title'] is not None:\n",
        "            combined_text = combined_text + \". \" +  row['link_preview_title']\n",
        "        if row['link_preview'] is not None:\n",
        "            combined_text = combined_text + \". \" +  row['link_preview']\n",
        "\n",
        "        combined_text = combined_text.replace('. . ','. ')\n",
        "\n",
        "        if combined_text is None:\n",
        "            continue\n",
        "\n",
        "        text = pp.prepare_single_text(combined_text)\n",
        "\n",
        "        if len(text.split(' ')) <= max_length:\n",
        "            # the text doesn't need to be splitted\n",
        "            # 412 is to make sure that we have fewer than 512 tokens\n",
        "            index, value = max(enumerate(ig.predict(text,tokenizer,model)), key=operator.itemgetter(1))\n",
        "            label = mp.model_params[selected_dataset]['mapping_reverse'][index]\n",
        "            confidence = value\n",
        "\n",
        "        else:\n",
        "            # text has too many tokens\n",
        "            # it has to be split\n",
        "            doc = nlp(combined_text)\n",
        "            sentences = [sent.text for sent in doc.sents]\n",
        "            texts = ['']\n",
        "            labels_confidences = []\n",
        "            pointer = 0\n",
        "            # create sentence buckets\n",
        "            for sentence in sentences:\n",
        "                try:\n",
        "                    text_preprocessed = pp.prepare_single_text(texts[pointer] + \" \" + sentence.strip())\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    print(str(sentence.text.strip()))\n",
        "                if len(text_preprocessed.split(' ')) <= max_length:\n",
        "                    texts[pointer] = texts[pointer] + \" \" + sentence.strip()\n",
        "                else:\n",
        "                    pointer = pointer + 1\n",
        "                    texts.append(sentence.strip())\n",
        "\n",
        "            \n",
        "            # predict each bucket\n",
        "            for bucket in texts:\n",
        "                index, confidence = max(enumerate(ig.predict(bucket,tokenizer,model)), key=operator.itemgetter(1))\n",
        "                label_idx = index\n",
        "\n",
        "                # As stated above, for the multiclass hasoc models we have to fix the mapping\n",
        "                if selected_dataset in ['hasoc_2020_task_2', 'hasoc_1920_task_2']:\n",
        "                  label_idx = hasoc_multiclass_map(label_idx)\n",
        "                labels_confidences.append([label_idx, confidence])\n",
        "\n",
        "            max_label_idx = -1\n",
        "            confidence = 0\n",
        "\n",
        "            # prefer the bucket that is classified as the most offensive\n",
        "            # the higher the label_idx, the more offensive is the message\n",
        "            # as the confidence we keep the highest confidece of the most offensive label\n",
        "            for prediction in labels_confidences:\n",
        "              label_idx = prediction[0]\n",
        "              if label_idx == max_label_idx:\n",
        "                confidence = max(confidence,prediction[1])\n",
        "              elif label_idx > max_label_idx:\n",
        "                max_label_idx = label_idx\n",
        "                confidence = prediction[1]\n",
        "\n",
        "            # now we have to \"undo\" the mapping (reverse the mapping)\n",
        "            if selected_dataset in ['hasoc_2020_task_2', 'hasoc_1920_task_2']:\n",
        "                  max_label_idx = hasoc_multiclass_map(max_label_idx)\n",
        "            label = mp.model_params[selected_dataset]['mapping_reverse'][max_label_idx]\n",
        "\n",
        "        # create update object\n",
        "        for objectid_to_update in lookup_table_hash_object_ids[row['hash']]:\n",
        "            update_query = UpdateOne({'_id': objectid_to_update},{'$set':{field_name:{'label':label,'confidence':confidence.item()}}}, upsert=True)\n",
        "            queue_done.put(1)\n",
        "            bulk.append(update_query)\n",
        "\n",
        "        # save to database\n",
        "        if len(bulk) >= 2000:\n",
        "            try:\n",
        "                result = db_messages.bulk_write(bulk)\n",
        "                bulk = []\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(bulk)  \n",
        "\n",
        "    # save the rest to database\n",
        "    try:\n",
        "        _ = db_messages.bulk_write(bulk)\n",
        "        bulk = []\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(bulk) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0YhVKAS6m80"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    field_name = f\"hate_{selected_dataset}\"\n",
        "    query={'language':'de', field_name : {'$exists': False}}\n",
        "\n",
        "    # get the messages from the database\n",
        "    df_messages = pd.DataFrame(list(db_messages.find(query,{'channel_name':1,'text':1,'link_preview_title':1,'link_preview':1})))\n",
        "    print(\"Number of all messages:\",len(df_messages))\n",
        "    time.sleep(1)\n",
        "    \n",
        "    # hash content for deduplicating\n",
        "    df_messages['hash'] = df_messages.progress_apply(lambda x: ig.hashContent(x['text'],x['link_preview_title'],x['link_preview']), axis=1)\n",
        "\n",
        "    # find distinct messages\n",
        "    df_messages_distinct = df_messages.drop_duplicates(subset=['hash'],keep='first')\n",
        "    print(\"Number of distinc messages:\", len(df_messages_distinct))\n",
        "    time.sleep(1)\n",
        "\n",
        "    # create lookup table\n",
        "    lookup_table_hash_object_ids = { i : [] for i in df_messages_distinct['hash'].to_list() }\n",
        "    for index,row in tqdm(df_messages.iterrows(), total=df_messages.shape[0]):\n",
        "        lookup_table_hash_object_ids[row['hash']].append(row['_id'])\n",
        "    \n",
        "    time.sleep(2) \n",
        "    # split dataframe into n chunks / n == number of workers\n",
        "    # one chunk for each worker\n",
        "    dfs = np.array_split(df_messages_distinct, workers)\n",
        "    \n",
        "    total = len(df_messages_distinct)\n",
        "    del [[df_messages,df_messages_distinct]]\n",
        "    gc.collect()\n",
        "    \n",
        "    df_messages = pd.DataFrame()\n",
        "    df_messages_distinct = pd.DataFrame()\n",
        "    \n",
        "    queue_done = multiprocessing.JoinableQueue()\n",
        "    \n",
        "    cur = 0\n",
        "\n",
        "    with tqdm(total=total, file=sys.stdout) as pbar:\n",
        "        jobs = []\n",
        "        # worker processes\n",
        "        for i in range(workers):\n",
        "            p = multiprocessing.Process(target=inferenceMessages, \n",
        "                                        args=(dfs[i],\n",
        "                                              queue_done,\n",
        "                                              selected_dataset,\n",
        "                                              lookup_table_hash_object_ids,\n",
        "                                             ))\n",
        "            p.daemon = True\n",
        "            p.start()\n",
        "            jobs.append(p)\n",
        "\n",
        "        while cur < total:\n",
        "            update = 0\n",
        "            while not queue_done.empty():\n",
        "                elem = queue_done.get()\n",
        "                update += elem\n",
        "            pbar.update(update)\n",
        "            cur += update\n",
        "            time.sleep(2)               \n",
        "        # update to 100%\n",
        "        pbar.update(total-cur)                       \n",
        "        print('Progress bar completed.')\n",
        "        # wait until all jobs are done\n",
        "        for job in jobs:\n",
        "            job.join()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}